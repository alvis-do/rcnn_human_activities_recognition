{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: import_ipynb in /home/vudhk/.local/lib/python3.6/site-packages (0.1.3)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --user import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import import_ipynb\n",
    "#from loss_function import loss_op\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 if using cudnn on GPU else using CPU\n",
    "CUDNN_GPU = 0\n",
    "model_path = 'my_model.csv'\n",
    "data_path = '../dataset/my_dict.npy'\n",
    "model_saver = '../model_saver/'\n",
    "n_classes = 4\n",
    "n_input = [224,224,3]\n",
    "n_output = [7,7,9]\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Layer</th>\n",
       "      <th>Filter</th>\n",
       "      <th>kernel</th>\n",
       "      <th>Stride</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Input</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>224x224x3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Conv</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3x3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>224x224x16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>MaxPooling</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2x2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>112x112x16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Conv</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3x3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>112x112x32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>MaxPooling</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2x2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>56x56x32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Conv</td>\n",
       "      <td>64.0</td>\n",
       "      <td>3x3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56x56x64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>MaxPooling</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2x2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>28x28x64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Conv</td>\n",
       "      <td>128.0</td>\n",
       "      <td>3x3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28x28x128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>MaxPooling</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2x2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14x14x128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Conv</td>\n",
       "      <td>256.0</td>\n",
       "      <td>3x3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14x14x256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>MaxPooling</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2x2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7x7x256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Conv</td>\n",
       "      <td>512.0</td>\n",
       "      <td>3x3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7x7x512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>Flatten</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25088x1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>Fc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4096x1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>Fc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>441x1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>Reshape</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7x7x9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0       Layer  Filter kernel  Stride      Output\n",
       "0            0       Input     NaN    NaN     NaN   224x224x3\n",
       "1            1        Conv    16.0    3x3     1.0  224x224x16\n",
       "2            2  MaxPooling     NaN    2x2     2.0  112x112x16\n",
       "3            3        Conv    32.0    3x3     1.0  112x112x32\n",
       "4            4  MaxPooling     NaN    2x2     2.0    56x56x32\n",
       "5            5        Conv    64.0    3x3     1.0    56x56x64\n",
       "6            6  MaxPooling     NaN    2x2     2.0    28x28x64\n",
       "7            7        Conv   128.0    3x3     1.0   28x28x128\n",
       "8            8  MaxPooling     NaN    2x2     2.0   14x14x128\n",
       "9            9        Conv   256.0    3x3     1.0   14x14x256\n",
       "10          10  MaxPooling     NaN    2x2     2.0     7x7x256\n",
       "11          11        Conv   512.0    3x3     1.0     7x7x512\n",
       "12          12     Flatten     NaN    NaN     NaN     25088x1\n",
       "13          13          Fc     NaN    NaN     NaN      4096x1\n",
       "14          14          Fc     NaN    NaN     NaN       441x1\n",
       "15          15     Reshape     NaN    NaN     NaN       7x7x9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load(data_path).item()\n",
    "pd.read_csv(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(input, model_path, is_training=True):\n",
    "    use_cudnn_on_gpu = True if CUDNN_GPU == 1 else False\n",
    "    my_model = pd.read_csv(model_path)\n",
    "    \n",
    "    ### Define some function for making layers\n",
    "    def make_input(input_, out_shape, name):\n",
    "        #result = tf.reshape(input_, [1,out_shape[0],out_shape[1],out_shape[2]], name=name)\n",
    "        result = input_/255.0\n",
    "        #tf.summary.histogram(name, result)\n",
    "        return result\n",
    "\n",
    "    def make_conv(input_, in_channel, out_channel, filter_, strides, name):\n",
    "        out_channel = int(out_channel)\n",
    "        strides = int(strides)\n",
    "        filter_ = list(map(int,filter_.split('x')))\n",
    "        filter_ = tf.Variable(tf.random_normal([filter_[0],filter_[1],in_channel,out_channel],stddev=0.1), name=name+'_filter')\n",
    "        # conv\n",
    "        result = tf.nn.conv2d(input=input_, \n",
    "                         filter=filter_,\n",
    "                         strides=[1, strides, strides, 1],\n",
    "                         padding='SAME',\n",
    "                         use_cudnn_on_gpu=use_cudnn_on_gpu,\n",
    "                         name=name)\n",
    "        # add bias\n",
    "        bias = tf.Variable(tf.random_normal([out_channel]))\n",
    "        result = tf.nn.bias_add(result, bias)\n",
    "        # relu\n",
    "        result = tf.nn.leaky_relu(result,alpha=0.1)\n",
    "        \n",
    "        #tf.summary.histogram(name, result)\n",
    "        return result\n",
    "        \n",
    "    def make_maxpool(input_, in_channel, filter_, strides, name):\n",
    "        strides = int(strides)\n",
    "        filter_ = list(map(int,filter_.split('x')))\n",
    "        result = tf.nn.max_pool(value=input_,\n",
    "                             ksize=[1, filter_[0], filter_[1], 1],\n",
    "                             strides=[1, strides, strides, 1],\n",
    "                             padding='SAME',\n",
    "                             name=name)\n",
    "        return result\n",
    "        \n",
    "    def make_flatten(input_, name):\n",
    "        return tf.contrib.layers.flatten(inputs=input_, scope=name)\n",
    "        \n",
    "    def make_fc(input_, out_shape, name):\n",
    "        result = tf.contrib.layers.fully_connected(inputs=input_,\n",
    "                                                    activation_fn=tf.nn.relu,\n",
    "                                                    num_outputs=int(out_shape[0]),\n",
    "                                                    scope=name)\n",
    "        #tf.summary.histogram(name, result)\n",
    "        return result\n",
    "    \n",
    "    def make_dropout(input_, name):\n",
    "        return tf.nn.dropout(input_, 0.75)\n",
    "    \n",
    "    def make_reshape(input_, out_shape, name):\n",
    "        result = tf.reshape(tensor=input_,\n",
    "                         shape=[tf.shape(input_)[0],out_shape[0],out_shape[1],out_shape[2]],\n",
    "                         name=name)\n",
    "        #tf.summary.histogram(name, result)\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    ### Generate the model base on the model file\n",
    "    output = input\n",
    "    layer_match = {\n",
    "            'Input':      lambda input_, params, _:       make_input(input_, params[5], 'input_image'),\n",
    "            'Conv':       lambda input_, params, channel: make_conv(input_, channel, params[2], params[3], params[4], 'layer_'+str(params[0])),\n",
    "            'MaxPooling': lambda input_, params, channel: make_maxpool(input_, channel, params[3], params[4], 'layer_'+str(params[0])),\n",
    "            'Flatten':    lambda input_, params, _:       make_flatten(input_, 'layer_'+str(params[0])),\n",
    "            'Fc':         lambda input_, params, _:       make_fc(input_, params[5], 'layer_'+str(params[0])),\n",
    "            'Reshape':    lambda input_, params, _:       make_reshape(input_, params[5], 'layer_'+str(params[0])),\n",
    "            'Dropout':    lambda input_, params, _:       make_dropout(input_, 'layer_'+str(params[0]))\n",
    "        }\n",
    "    prev_channel = None\n",
    "    for layer in my_model.values:\n",
    "        # preprocessing layer input\n",
    "        layer[-1] = np.array(list(map(int,layer[-1].split('x'))))\n",
    "        # map layer\n",
    "        output = layer_match[layer[1]](output, layer, prev_channel)\n",
    "        prev_channel = layer[-1][-1]\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(X, weights, biases, isTraining, num_classes):\n",
    "    # Preprocess data input\n",
    "    \n",
    "    # Create LSTM cell\n",
    "    lstm_cell = None;\n",
    "    if CUDNN_GPU == 0:\n",
    "        lstm_cell = tf.contrib.rnn.LSTMBlockCell(n_hidden, \n",
    "                                                forget_bias=1.0)\n",
    "    else:\n",
    "        lstm_cell = tf.contrib.cudnn_rnn.CudnnLSTM(num_layers=1, \n",
    "                                                   num_units=n_hidden,\n",
    "                                                   kernel_initializer=tf.initializers.random_uniform(-0.01, 0.01),\n",
    "                                                   bias_initializer=tf.initializers.constant(0))\n",
    "    \n",
    "    # Creates a recurrent neural network specified by RNNCell cell.\n",
    "    lstm_out, _ = tf.contrib.rnn.static_rnn(cell=lstm_cell, \n",
    "                                            inputs=X,\n",
    "                                            dtype=tf.float32)\n",
    "    # Dropout layer\n",
    "    dropout = tf.layers.dropout(inputs=lstm_out, \n",
    "                               rate=0.5,\n",
    "                               training=isTraining)\n",
    "    \n",
    "    # Fully connected layer\n",
    "    # weights_initializer is gaussian distribution\n",
    "    # bias_initializer is constant by zero\n",
    "    fc = tf.contrib.layers.fully_connected(inputs=dropout,\n",
    "                                            num_outputs=num_classes,\n",
    "                                            activation_fn=None,\n",
    "                                            weights_initializer=tf.initializers.truncated_normal(stddev=0.01),\n",
    "                                            bias_initializer=tf.initializers.constant(0))\n",
    "    \n",
    "    # Batch Norm + Scale layer\n",
    "    batch_norm = tf.layers.batch_normalization(inputs=fc,\n",
    "                                                axis=2,\n",
    "                                                training=isTraining)\n",
    "    \n",
    "    # ReLU activation\n",
    "    relu = tf.nn.relu_layer(batch_norm)\n",
    "    \n",
    "    lstm_last_output = outputs[-1]\n",
    "    return lstm_last_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = data['X_train'].shape[0]\n",
    "test_len = data['X_test'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# the placeholders will be used for the feed_dict param of tf.Session.run()\n",
    "is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_input[0], n_input[1], n_input[2]], name='X_train')\n",
    "y = tf.placeholder(tf.float32, [None, n_output[0], n_output[1], n_output[2]], name='y_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMDA_NOOBJ = 0.5\n",
    "LAMDA_COORD = 5.0\n",
    "\n",
    "def loss_op(y_pred, y_true):\n",
    "    mask_shape = tf.shape(y_true)[:3]\n",
    "    \n",
    "    cell_x = tf.to_float(tf.reshape(tf.tile(tf.range(7), [7]), (1, 7, 7, 1)))\n",
    "    cell_y = tf.transpose(cell_x, (0,2,1,3))\n",
    "    cell_grid = tf.tile(tf.concat([cell_x,cell_y], -1), [batch_size, 1, 1, 1])\n",
    "\n",
    "    coord_mask = tf.zeros(mask_shape)\n",
    "    conf_mask  = tf.zeros(mask_shape)\n",
    "    class_mask = tf.zeros(mask_shape)\n",
    "\n",
    "    seen = tf.Variable(0.)\n",
    "    total_recall = tf.Variable(0.)\n",
    "\n",
    "    \"\"\"\n",
    "    Adjust prediction\n",
    "    \"\"\"\n",
    "    ### adjust x and y      \n",
    "    pred_box_xy = tf.sigmoid(y_pred[..., :2]) + cell_grid\n",
    "\n",
    "    ### adjust w and h\n",
    "    pred_box_wh = tf.exp(tf.sigmoid(y_pred[..., 2:4]))\n",
    "    #tf.summary.scalar(\"pred_box_wh_1\", tf.reduce_max(y_pred[..., 2:4]))\n",
    "\n",
    "    ### adjust confidence\n",
    "    pred_box_conf = tf.sigmoid(y_pred[..., 4])\n",
    "\n",
    "    ### adjust class probabilities\n",
    "    pred_box_class = y_pred[..., 5:]\n",
    "\n",
    "    \"\"\"\n",
    "    Adjust ground truth\n",
    "    \"\"\"\n",
    "    ### adjust x and y\n",
    "    true_box_xy = y_true[..., 0:2] # relative position to the containing cell\n",
    "\n",
    "    ### adjust w and h\n",
    "    true_box_wh = y_true[..., 2:4] # number of cells accross, horizontally and vertically\n",
    "\n",
    "    ### adjust confidence\n",
    "    true_wh_half = true_box_wh / 2.\n",
    "    true_mins    = true_box_xy - true_wh_half\n",
    "    true_maxes   = true_box_xy + true_wh_half\n",
    "\n",
    "    pred_wh_half = pred_box_wh / 2.\n",
    "    pred_mins    = pred_box_xy - pred_wh_half\n",
    "    pred_maxes   = pred_box_xy + pred_wh_half       \n",
    "\n",
    "    intersect_mins  = tf.maximum(pred_mins,  true_mins)\n",
    "    intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
    "    intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "\n",
    "    true_areas = true_box_wh[..., 0] * true_box_wh[..., 1]\n",
    "    pred_areas = pred_box_wh[..., 0] * pred_box_wh[..., 1]\n",
    "\n",
    "    union_areas = pred_areas + true_areas - intersect_areas\n",
    "    iou_scores  = tf.truediv(intersect_areas, union_areas)\n",
    "\n",
    "    true_box_conf = iou_scores * y_true[..., 4]\n",
    "\n",
    "    ### adjust class probabilities\n",
    "    true_box_class = tf.argmax(y_true[..., 5:], -1)\n",
    "\n",
    "    \"\"\"\n",
    "    Determine the masks\n",
    "    \"\"\"\n",
    "    ### coordinate mask: simply the position of the ground truth boxes (the predictors)\n",
    "    coord_mask = tf.expand_dims(y_true[..., 4], axis=-1)\n",
    "\n",
    "    ### confidence mask: penalize predictors + penalize boxes with low IOU\n",
    "    conf_noobj_mask = 1 - y_true[..., 4]\n",
    "\n",
    "    # penalize the confidence of the boxes, which are reponsible for corresponding ground truth box\n",
    "    conf_obj_mask = y_true[..., 4]\n",
    "\n",
    "    ### class mask: simply the position of the ground truth boxes (the predictors)\n",
    "    class_mask = y_true[..., 4] * tf.ones(tf.shape(y_true[..., 4])) \n",
    "\n",
    "    \"\"\"\n",
    "    Finalize the loss\n",
    "    \"\"\"\n",
    "    loss_xy    = tf.reduce_sum(tf.square(true_box_xy-pred_box_xy)           * coord_mask)\n",
    "    loss_wh    = tf.reduce_sum(tf.square(true_box_wh-pred_box_wh)           * coord_mask)\n",
    "    loss_conf_obj  = tf.reduce_sum(tf.square(true_box_conf-pred_box_conf)   * conf_obj_mask) \n",
    "    loss_conf_noobj  = tf.reduce_sum(tf.square(true_box_conf-pred_box_conf) * conf_noobj_mask) \n",
    "    loss_class = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=true_box_class, logits=pred_box_class)\n",
    "    loss_class = tf.reduce_sum(loss_class * class_mask)\n",
    "\n",
    "    loss = (loss_xy + loss_wh) * LAMDA_COORD + loss_conf_obj + loss_conf_noobj * LAMDA_NOOBJ + loss_class\n",
    "    #loss = loss_wh\n",
    "\n",
    "    nb_true_box = tf.reduce_sum(y_true[..., 4])\n",
    "    nb_pred_box = tf.reduce_sum(tf.to_float(true_box_conf > 0.5) * tf.to_float(pred_box_conf > 0.3))\n",
    "\n",
    "    \"\"\"\n",
    "    Debugging code\n",
    "    \"\"\"    \n",
    "    current_recall = nb_pred_box/(nb_true_box + 1e-6)\n",
    "    total_recall = tf.assign_add(total_recall, current_recall) \n",
    "\n",
    "    loss = tf.Print(loss, [loss_xy], message='Loss XY \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_wh], message='Loss WH \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_conf_obj], message='Loss Conf Obj \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_conf_noobj], message='Loss Conf Noobj \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_class], message='Loss Class \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss], message='Total Loss \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [current_recall], message='Current Recall \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [total_recall/seen], message='Average Recall \\t', summarize=1000)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "cnn_model = cnn_model(X, model_path, is_training)\n",
    "\n",
    "# learning rate\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "#learning_rate = tf.train.exponential_decay(\n",
    "#                        0.001,  # Base learning rate.\n",
    "#                        global_step,  # Current index into the dataset.\n",
    "#                        train_len,  # Decay step.\n",
    "#                        0.95,  # Decay rate.\n",
    "#                        staircase=True,\n",
    "#                        name='learning_rate')\n",
    "learning_rate = 0.0005\n",
    "# loss \n",
    "loss_op = loss_op(cnn_model, y);\n",
    "# optimizer \n",
    "optimal = tf.train.AdamOptimizer(learning_rate, name='adam_optimizer')\n",
    "# increment global_step at each step.\n",
    "train_op = optimal.minimize(loss_op, name='optimal_min')\n",
    "\n",
    "# evaluate model\n",
    "correct_prediction = tf.equal(tf.argmax(cnn_model, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tao summary cua cac monitor de quan sat cac bien\n",
    "tf.summary.scalar('loss_op', loss_op)\n",
    "tf.summary.scalar('learning_rate', learning_rate)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# gop cac summaries vao mot operation\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "# tao doi tuong log writer va ghi vao Tensorboard\n",
    "tf_writer = tf.summary.FileWriter('../checkpoint', graph=tf.get_default_graph())\n",
    "\n",
    "# khoi tao cac variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(dict_field_name, batch):\n",
    "    return data[dict_field_name][batch*batch_size:min((batch+1)*batch_size,train_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Batch:0000,cost={6832.280761719}, training accuracy 0.18204\n",
      "\n",
      "---Batch:0001,cost={12618.822265625}, training accuracy 0.15526\n",
      "\n",
      "---Batch:0002,cost={8532.790039062}, training accuracy 0.14683\n",
      "\n",
      "---Batch:0003,cost={8782.526367188}, training accuracy 0.17808\n",
      "\n",
      "---Batch:0004,cost={7843.971679688}, training accuracy 0.21081\n",
      "\n",
      "---Batch:0005,cost={6778.716308594}, training accuracy 0.23859\n",
      "\n",
      "---Batch:0006,cost={5760.497070312}, training accuracy 0.25744\n",
      "\n",
      "---Batch:0007,cost={6387.741210938}, training accuracy 0.24256\n",
      "\n",
      "---Batch:0008,cost={5586.432617188}, training accuracy 0.23264\n",
      "\n",
      "---Batch:0009,cost={6495.916503906}, training accuracy 0.23760\n",
      "\n",
      "---Batch:0010,cost={5793.320312500}, training accuracy 0.27331\n",
      "\n",
      "---Batch:0011,cost={6715.105468750}, training accuracy 0.28919\n",
      "\n",
      "---Batch:0012,cost={6174.022949219}, training accuracy 0.25744\n",
      "\n",
      "---Batch:0013,cost={5534.975585938}, training accuracy 0.27431\n",
      "\n",
      "---Batch:0014,cost={6659.664550781}, training accuracy 0.33135\n",
      "\n",
      "---Batch:0015,cost={4985.574218750}, training accuracy 0.32292\n",
      "\n",
      "---Batch:0016,cost={4784.875488281}, training accuracy 0.33581\n",
      "\n",
      "---Batch:0017,cost={5799.565917969}, training accuracy 0.34623\n",
      "\n",
      "---Batch:0018,cost={4938.778320312}, training accuracy 0.36210\n",
      "\n",
      "---Batch:0019,cost={5450.390625000}, training accuracy 0.35962\n",
      "\n",
      "---Batch:0020,cost={5246.892578125}, training accuracy 0.39633\n",
      "\n",
      "---Batch:0021,cost={5054.677734375}, training accuracy 0.40476\n",
      "\n",
      "---Batch:0022,cost={4768.384277344}, training accuracy 0.37847\n",
      "\n",
      "---Batch:0023,cost={4952.190917969}, training accuracy 0.35863\n",
      "\n",
      "---Batch:0024,cost={4996.669921875}, training accuracy 0.36806\n",
      "\n",
      "---Batch:0025,cost={4505.520507812}, training accuracy 0.38194\n",
      "\n",
      "---Batch:0026,cost={6011.087890625}, training accuracy 0.35665\n",
      "\n",
      "---Batch:0027,cost={6546.432128906}, training accuracy 0.36161\n",
      "\n",
      "---Batch:0028,cost={5045.590332031}, training accuracy 0.36508\n",
      "\n",
      "---Batch:0029,cost={5135.754394531}, training accuracy 0.35516\n",
      "\n",
      "---Batch:0030,cost={5726.043457031}, training accuracy 0.34474\n",
      "\n",
      "---Batch:0031,cost={4673.221679688}, training accuracy 0.32937\n",
      "\n",
      "---Batch:0032,cost={5432.207031250}, training accuracy 0.34573\n",
      "\n",
      "---Batch:0033,cost={5609.056640625}, training accuracy 0.35615\n",
      "\n",
      "---Batch:0034,cost={4695.087402344}, training accuracy 0.35962\n",
      "\n",
      "---Batch:0035,cost={5239.783203125}, training accuracy 0.36260\n",
      "\n",
      "---Batch:0036,cost={5109.261718750}, training accuracy 0.37252\n",
      "\n",
      "---Batch:0037,cost={5462.166015625}, training accuracy 0.36706\n",
      "\n",
      "---Batch:0038,cost={5005.850097656}, training accuracy 0.37897\n",
      "\n",
      "---Batch:0039,cost={5114.726562500}, training accuracy 0.37054\n",
      "\n",
      "---Batch:0040,cost={5935.263183594}, training accuracy 0.36508\n",
      "\n",
      "---Batch:0041,cost={4744.070800781}, training accuracy 0.37450\n",
      "\n",
      "---Batch:0042,cost={5488.867187500}, training accuracy 0.37599\n",
      "\n",
      "---Batch:0043,cost={5935.455566406}, training accuracy 0.37897\n",
      "\n",
      "---Batch:0044,cost={5748.317382812}, training accuracy 0.37649\n",
      "\n",
      "---Batch:0045,cost={4957.500488281}, training accuracy 0.35417\n",
      "\n",
      "---Batch:0046,cost={6158.426757812}, training accuracy 0.35565\n",
      "\n",
      "---Batch:0047,cost={4741.022460938}, training accuracy 0.35367\n",
      "\n",
      "---Batch:0048,cost={6207.995117188}, training accuracy 0.36458\n",
      "\n",
      "---Batch:0049,cost={5062.308105469}, training accuracy 0.36756\n",
      "\n",
      "---Batch:0050,cost={5757.980957031}, training accuracy 0.37847\n",
      "\n",
      "---Batch:0051,cost={6289.680664062}, training accuracy 0.39831\n",
      "\n",
      "---Batch:0052,cost={5431.287597656}, training accuracy 0.41121\n",
      "\n",
      "---Batch:0053,cost={4688.421875000}, training accuracy 0.39534\n",
      "\n",
      "Epoch:0001,cost={4688.421875000}, training accuracy 0.39534\n",
      "\n",
      "Model saved in path: ../model_saver/nn_model_0001.ckpt \n",
      "\n",
      "---Batch:0000,cost={5229.665039062}, training accuracy 0.40526\n",
      "\n",
      "---Batch:0001,cost={5264.065429688}, training accuracy 0.42212\n",
      "\n",
      "---Batch:0002,cost={4447.139648438}, training accuracy 0.43105\n",
      "\n",
      "---Batch:0003,cost={5431.744140625}, training accuracy 0.39831\n",
      "\n",
      "---Batch:0004,cost={5430.664550781}, training accuracy 0.52579\n",
      "\n",
      "---Batch:0005,cost={5220.917968750}, training accuracy 0.55952\n",
      "\n",
      "---Batch:0006,cost={4781.827148438}, training accuracy 0.52679\n",
      "\n",
      "---Batch:0007,cost={4968.138183594}, training accuracy 0.51538\n",
      "\n",
      "---Batch:0008,cost={4677.031738281}, training accuracy 0.51687\n",
      "\n",
      "---Batch:0009,cost={5439.322265625}, training accuracy 0.50744\n",
      "\n",
      "---Batch:0010,cost={4895.689453125}, training accuracy 0.50794\n",
      "\n",
      "---Batch:0011,cost={5745.928710938}, training accuracy 0.49950\n",
      "\n",
      "---Batch:0012,cost={5604.828125000}, training accuracy 0.49504\n",
      "\n",
      "---Batch:0013,cost={5133.643554688}, training accuracy 0.49355\n",
      "\n",
      "---Batch:0014,cost={6108.646484375}, training accuracy 0.49157\n",
      "\n",
      "---Batch:0015,cost={4620.909667969}, training accuracy 0.47173\n",
      "\n",
      "---Batch:0016,cost={4594.796386719}, training accuracy 0.45536\n",
      "\n",
      "---Batch:0017,cost={5615.683105469}, training accuracy 0.45486\n",
      "\n",
      "---Batch:0018,cost={4730.106933594}, training accuracy 0.45982\n",
      "\n",
      "---Batch:0019,cost={5115.827636719}, training accuracy 0.44692\n",
      "\n",
      "---Batch:0020,cost={5181.718750000}, training accuracy 0.45040\n",
      "\n",
      "---Batch:0021,cost={4948.343750000}, training accuracy 0.48462\n",
      "\n",
      "---Batch:0022,cost={4658.403320312}, training accuracy 0.48611\n",
      "\n",
      "---Batch:0023,cost={4920.575683594}, training accuracy 0.48016\n",
      "\n",
      "---Batch:0024,cost={4909.162109375}, training accuracy 0.48115\n",
      "\n",
      "---Batch:0025,cost={4434.620605469}, training accuracy 0.47272\n",
      "\n",
      "---Batch:0026,cost={5907.974609375}, training accuracy 0.47321\n",
      "\n",
      "---Batch:0027,cost={6014.465820312}, training accuracy 0.46032\n",
      "\n",
      "---Batch:0028,cost={4980.390136719}, training accuracy 0.47718\n",
      "\n",
      "---Batch:0029,cost={5018.511718750}, training accuracy 0.48859\n",
      "\n",
      "---Batch:0030,cost={5493.554687500}, training accuracy 0.48462\n",
      "\n",
      "---Batch:0031,cost={4597.795898438}, training accuracy 0.48016\n",
      "\n",
      "---Batch:0032,cost={5345.902832031}, training accuracy 0.48165\n",
      "\n",
      "---Batch:0033,cost={5428.469726562}, training accuracy 0.48065\n",
      "\n",
      "---Batch:0034,cost={4638.748046875}, training accuracy 0.48462\n",
      "\n",
      "---Batch:0035,cost={5199.967285156}, training accuracy 0.48710\n",
      "\n",
      "---Batch:0036,cost={4981.410644531}, training accuracy 0.47917\n",
      "\n",
      "---Batch:0037,cost={5353.846679688}, training accuracy 0.47421\n",
      "\n",
      "---Batch:0038,cost={4816.222167969}, training accuracy 0.48661\n",
      "\n",
      "---Batch:0039,cost={5038.754394531}, training accuracy 0.47817\n",
      "\n",
      "---Batch:0040,cost={5929.009765625}, training accuracy 0.47520\n",
      "\n",
      "---Batch:0041,cost={4739.363281250}, training accuracy 0.48165\n",
      "\n",
      "---Batch:0042,cost={5853.814453125}, training accuracy 0.49206\n",
      "\n",
      "---Batch:0043,cost={5618.903320312}, training accuracy 0.48661\n",
      "\n",
      "---Batch:0044,cost={5716.029785156}, training accuracy 0.47669\n",
      "\n",
      "---Batch:0045,cost={4955.717773438}, training accuracy 0.47520\n",
      "\n",
      "---Batch:0046,cost={6159.812988281}, training accuracy 0.44692\n",
      "\n",
      "---Batch:0047,cost={4631.614257812}, training accuracy 0.45437\n",
      "\n",
      "---Batch:0048,cost={6209.316894531}, training accuracy 0.46776\n",
      "\n",
      "---Batch:0049,cost={5062.308105469}, training accuracy 0.46230\n",
      "\n",
      "---Batch:0050,cost={5561.186523438}, training accuracy 0.47718\n",
      "\n",
      "---Batch:0051,cost={6266.067382812}, training accuracy 0.48214\n",
      "\n",
      "---Batch:0052,cost={5498.327148438}, training accuracy 0.48115\n",
      "\n",
      "---Batch:0053,cost={4688.421875000}, training accuracy 0.49058\n",
      "\n",
      "Epoch:0002,cost={4688.421875000}, training accuracy 0.49058\n",
      "\n",
      "Model saved in path: ../model_saver/nn_model_0002.ckpt \n",
      "\n",
      "---Batch:0000,cost={5164.984863281}, training accuracy 0.49107\n",
      "\n",
      "---Batch:0001,cost={5273.043945312}, training accuracy 0.50000\n",
      "\n",
      "---Batch:0002,cost={4429.325683594}, training accuracy 0.53323\n",
      "\n",
      "---Batch:0003,cost={5244.676757812}, training accuracy 0.52530\n",
      "\n",
      "---Batch:0004,cost={5399.903808594}, training accuracy 0.49950\n",
      "\n",
      "---Batch:0005,cost={5171.579101562}, training accuracy 0.50744\n",
      "\n",
      "---Batch:0006,cost={4763.063476562}, training accuracy 0.50546\n",
      "\n",
      "---Batch:0007,cost={4974.738281250}, training accuracy 0.50546\n",
      "\n",
      "---Batch:0008,cost={4677.031738281}, training accuracy 0.51141\n",
      "\n",
      "---Batch:0009,cost={5439.322265625}, training accuracy 0.52530\n",
      "\n",
      "---Batch:0010,cost={4895.390136719}, training accuracy 0.52232\n",
      "\n",
      "---Batch:0011,cost={5686.751953125}, training accuracy 0.52728\n",
      "\n",
      "---Batch:0012,cost={5749.191406250}, training accuracy 0.52927\n",
      "\n",
      "---Batch:0013,cost={5276.147949219}, training accuracy 0.52778\n",
      "\n",
      "---Batch:0014,cost={6112.350097656}, training accuracy 0.50248\n",
      "\n",
      "---Batch:0015,cost={4620.909667969}, training accuracy 0.50893\n",
      "\n",
      "---Batch:0016,cost={4593.952148438}, training accuracy 0.49405\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Batch:0017,cost={5604.184082031}, training accuracy 0.47421\n",
      "\n",
      "---Batch:0018,cost={4688.659179688}, training accuracy 0.48413\n",
      "\n",
      "---Batch:0019,cost={5103.925292969}, training accuracy 0.47173\n",
      "\n",
      "---Batch:0020,cost={4857.295410156}, training accuracy 0.50397\n",
      "\n",
      "---Batch:0021,cost={4879.206542969}, training accuracy 0.51538\n",
      "\n",
      "---Batch:0022,cost={4616.955566406}, training accuracy 0.51984\n",
      "\n",
      "---Batch:0023,cost={4886.441406250}, training accuracy 0.49306\n",
      "\n",
      "---Batch:0024,cost={4875.813964844}, training accuracy 0.51141\n",
      "\n",
      "---Batch:0025,cost={4422.620605469}, training accuracy 0.51141\n",
      "\n",
      "---Batch:0026,cost={5889.623535156}, training accuracy 0.51637\n",
      "\n",
      "---Batch:0027,cost={5968.868164062}, training accuracy 0.50744\n",
      "\n",
      "---Batch:0028,cost={4968.015136719}, training accuracy 0.51885\n",
      "\n",
      "---Batch:0029,cost={5001.129394531}, training accuracy 0.53075\n",
      "\n",
      "---Batch:0030,cost={5481.179687500}, training accuracy 0.53621\n",
      "\n",
      "---Batch:0031,cost={4555.959960938}, training accuracy 0.53522\n",
      "\n",
      "---Batch:0032,cost={5334.738769531}, training accuracy 0.54464\n",
      "\n",
      "---Batch:0033,cost={5390.488281250}, training accuracy 0.55308\n",
      "\n",
      "---Batch:0034,cost={4599.740722656}, training accuracy 0.57242\n",
      "\n",
      "---Batch:0035,cost={5189.144531250}, training accuracy 0.56300\n",
      "\n",
      "---Batch:0036,cost={5008.218750000}, training accuracy 0.54812\n",
      "\n",
      "---Batch:0037,cost={5311.948242188}, training accuracy 0.54216\n",
      "\n",
      "---Batch:0038,cost={4806.607421875}, training accuracy 0.55704\n",
      "\n",
      "---Batch:0039,cost={5014.324707031}, training accuracy 0.54861\n",
      "\n",
      "---Batch:0040,cost={5838.236328125}, training accuracy 0.54415\n",
      "\n",
      "---Batch:0041,cost={4701.826171875}, training accuracy 0.54762\n",
      "\n",
      "---Batch:0042,cost={5214.510253906}, training accuracy 0.54415\n",
      "\n",
      "---Batch:0043,cost={5522.186035156}, training accuracy 0.54315\n",
      "\n",
      "---Batch:0044,cost={5403.834472656}, training accuracy 0.53621\n",
      "\n",
      "---Batch:0045,cost={4921.860839844}, training accuracy 0.53571\n",
      "\n",
      "---Batch:0046,cost={6084.800781250}, training accuracy 0.54812\n",
      "\n",
      "---Batch:0047,cost={4614.325195312}, training accuracy 0.55804\n",
      "\n",
      "---Batch:0048,cost={6142.416015625}, training accuracy 0.56548\n",
      "\n",
      "---Batch:0049,cost={5001.469238281}, training accuracy 0.56895\n",
      "\n",
      "---Batch:0050,cost={5460.188476562}, training accuracy 0.57788\n",
      "\n",
      "---Batch:0051,cost={6182.853515625}, training accuracy 0.56895\n",
      "\n",
      "---Batch:0052,cost={5393.100585938}, training accuracy 0.57093\n",
      "\n",
      "---Batch:0053,cost={4679.088378906}, training accuracy 0.56944\n",
      "\n",
      "Epoch:0003,cost={4679.088378906}, training accuracy 0.56944\n",
      "\n",
      "Model saved in path: ../model_saver/nn_model_0003.ckpt \n",
      "\n",
      "---Batch:0000,cost={5059.604492188}, training accuracy 0.56151\n",
      "\n",
      "---Batch:0001,cost={5217.334472656}, training accuracy 0.57093\n",
      "\n",
      "---Batch:0002,cost={4358.376953125}, training accuracy 0.57540\n",
      "\n",
      "---Batch:0003,cost={5232.301757812}, training accuracy 0.56597\n",
      "\n",
      "---Batch:0004,cost={5357.056152344}, training accuracy 0.56696\n",
      "\n",
      "---Batch:0005,cost={5159.579101562}, training accuracy 0.57440\n",
      "\n",
      "---Batch:0006,cost={4750.688476562}, training accuracy 0.56597\n",
      "\n",
      "---Batch:0007,cost={4905.913574219}, training accuracy 0.56895\n",
      "\n",
      "---Batch:0008,cost={4665.031738281}, training accuracy 0.56845\n",
      "\n",
      "---Batch:0009,cost={5404.936035156}, training accuracy 0.56696\n",
      "\n",
      "---Batch:0010,cost={4861.807617188}, training accuracy 0.56548\n",
      "\n",
      "---Batch:0011,cost={5645.655273438}, training accuracy 0.56647\n",
      "\n",
      "---Batch:0012,cost={5543.599609375}, training accuracy 0.56647\n",
      "\n",
      "---Batch:0013,cost={5094.213867188}, training accuracy 0.56746\n",
      "\n",
      "---Batch:0014,cost={6065.864746094}, training accuracy 0.56944\n",
      "\n",
      "---Batch:0015,cost={4598.995117188}, training accuracy 0.57292\n",
      "\n",
      "---Batch:0016,cost={4582.796386719}, training accuracy 0.55952\n",
      "\n",
      "---Batch:0017,cost={5581.183593750}, training accuracy 0.56250\n",
      "\n",
      "---Batch:0018,cost={4689.731933594}, training accuracy 0.56696\n",
      "\n",
      "---Batch:0019,cost={5078.054199219}, training accuracy 0.55556\n",
      "\n",
      "---Batch:0020,cost={4802.857421875}, training accuracy 0.55754\n",
      "\n",
      "---Batch:0021,cost={4882.228027344}, training accuracy 0.56498\n",
      "\n",
      "---Batch:0022,cost={4616.955566406}, training accuracy 0.56845\n",
      "\n",
      "---Batch:0023,cost={4857.255371094}, training accuracy 0.55804\n",
      "\n",
      "---Batch:0024,cost={4875.813964844}, training accuracy 0.56101\n",
      "\n",
      "---Batch:0025,cost={4422.620605469}, training accuracy 0.55952\n",
      "\n",
      "---Batch:0026,cost={5890.696289062}, training accuracy 0.56796\n",
      "\n",
      "---Batch:0027,cost={5948.234863281}, training accuracy 0.55506\n",
      "\n",
      "---Batch:0028,cost={4968.015136719}, training accuracy 0.55407\n",
      "\n",
      "---Batch:0029,cost={5003.306152344}, training accuracy 0.56647\n",
      "\n",
      "---Batch:0030,cost={5481.179687500}, training accuracy 0.56498\n",
      "\n",
      "---Batch:0031,cost={4557.147949219}, training accuracy 0.55407\n",
      "\n",
      "---Batch:0032,cost={5307.450683594}, training accuracy 0.56002\n",
      "\n",
      "---Batch:0033,cost={5390.488281250}, training accuracy 0.55308\n",
      "\n",
      "---Batch:0034,cost={4593.038085938}, training accuracy 0.56052\n",
      "\n",
      "---Batch:0035,cost={5189.144531250}, training accuracy 0.56498\n",
      "\n",
      "---Batch:0036,cost={4968.241699219}, training accuracy 0.55456\n",
      "\n",
      "---Batch:0037,cost={5311.948242188}, training accuracy 0.54861\n",
      "\n",
      "---Batch:0038,cost={4806.607421875}, training accuracy 0.56498\n",
      "\n",
      "---Batch:0039,cost={5014.324707031}, training accuracy 0.55407\n",
      "\n",
      "---Batch:0040,cost={5838.236328125}, training accuracy 0.55010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove model_saver folder\n",
    "shutil.rmtree(model_saver, ignore_errors=True)\n",
    "\n",
    "# training\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    sess.run(init, feed_dict={is_training:True})\n",
    "    # Training cycle\n",
    "    epoch = 0;\n",
    "    while True:\n",
    "        total_batch =  train_len // batch_size\n",
    "        for batch in range(total_batch):\n",
    "            # lay batch tiep theo\n",
    "            batch_input = get_batch('X_train', batch) \n",
    "            batch_label = get_batch('y_train', batch)\n",
    "            # chay train_op, loss_op, accuracy\n",
    "            _, cost, acc, summary = sess.run([train_op, loss_op, accuracy, merged_summary_op], feed_dict={X:batch_input, y:batch_label, is_training:True})\n",
    "            # Write logs at every iteration\n",
    "            tf_writer.add_summary(summary, epoch * total_batch + batch)\n",
    "            print(\"---Batch:\" + ('%04d,' % (batch)) + (\"cost={%.9f}, training accuracy %.5f\" % (cost, acc)) + \"\\n\")\n",
    "\n",
    "        epoch += 1;\n",
    "        \n",
    "        # hien thi ket qua sau moi epoch\n",
    "        print(\"Epoch:\" + ('%04d,' % (epoch)) + (\"cost={%.9f}, training accuracy %.5f\" % (cost, acc)) + \"\\n\")\n",
    "        \n",
    "        if epoch % 1 == 0:\n",
    "            # Luu tru variables vao disk.\n",
    "            save_path = saver.save(sess, model_saver + 'nn_model_%04d.ckpt'%(epoch))\n",
    "            print(\"Model saved in path: %s \\n\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "# TODO: Change the eval_epoch_num variable by a suitable number of epoch.\n",
    "eval_epoch_num = 300\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, model_saver + 'nn_model_%04d.ckpt'%(eval_epoch_num))\n",
    "    avg_acc = 0.\n",
    "    total_batch =  test_len // batch_size\n",
    "    for batch in range(total_batch):\n",
    "        # get next batch\n",
    "        batch_input = get_batch('X_test', batch) \n",
    "        batch_label = get_batch('y_test', batch) \n",
    "        acc = sess.run(accuracy, feed_dict={X:batch_input, y:batch_label, is_training:False})\n",
    "        avg_acc += acc / total_batch\n",
    "    print(\"Accuracy on test set: %.5f \\n\" % (avg_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
